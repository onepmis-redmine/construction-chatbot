#ì½”íŒŒì¼ëŸ¿ì´ ì¶”ì²œí•´ì¤€ ìˆ˜ì •ëœ ì½”ë“œ
import os
import json
import chromadb
import torch
from transformers import AutoTokenizer, AutoModel
from tqdm import tqdm
from pathlib import Path


# í´ë” ê²½ë¡œ
CHUNKS_DIR = "chunks/"
DB_DIR = "vector_db/"


# ì´ ë¶€ë¶„ ì¶”ê°€!
chunk_dir = Path("chunks")
chunk_paths = sorted(chunk_dir.glob("*.txt"))


# ğŸ›  í´ë” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
if not os.path.exists(CHUNKS_DIR):
    print(f"âš ï¸ í´ë” '{CHUNKS_DIR}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    exit(1)

os.makedirs(DB_DIR, exist_ok=True)  # ë²¡í„°DB ì €ì¥í•  í´ë” ìƒì„±

# ChromaDB í´ë¼ì´ì–¸íŠ¸ ìƒì„±
chroma_client = chromadb.PersistentClient(path=DB_DIR)

# ë²¡í„°DB Collection ìƒì„±
collection = chroma_client.get_or_create_collection(name="construction_manuals")

# ì„ë² ë”© ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
model_name = "sentence-transformers/all-MiniLM-L6-v2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

# Mean Pooling í•¨ìˆ˜
def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output[0]
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

# í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def get_embeddings(texts):
    # ì…ë ¥ì´ ë¬¸ìì—´ì¸ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    if isinstance(texts, str):
        texts = [texts]
    
    # í† í¬ë‚˜ì´ì¦ˆ ë° ëª¨ë¸ ì…ë ¥ ì¤€ë¹„
    encoded_input = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors='pt')
    
    # ëª¨ë¸ ì¶”ë¡ 
    with torch.no_grad():
        model_output = model(**encoded_input)
    
    # Mean Pooling
    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])
    
    # ì •ê·œí™”
    sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)
    
    return sentence_embeddings[0].tolist() if len(texts) == 1 else sentence_embeddings.tolist()

# ë¬¸ë‹¨ ì½ê³  ì„ë² ë”©í•´ì„œ ì €ì¥
batch_size = 32  # âœ… ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ ìµœì í™”
total_added = 0

for filename in tqdm(os.listdir(CHUNKS_DIR)):
    if not filename.endswith(".json"):
        continue

    filepath = os.path.join(CHUNKS_DIR, filename)

    with open(filepath, "r", encoding="utf-8") as f:
        chunks = json.load(f)

    texts = [chunk["text"] for chunk in chunks]
    ids = [f"{filename}_{i}" for i in range(len(texts))]

    # ë°°ì¹˜ ì²˜ë¦¬
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i + batch_size]
        batch_ids = ids[i:i + batch_size]
        
        embeddings = get_embeddings(batch_texts)
        
        collection.add(
            documents=batch_texts,
            embeddings=embeddings,
            ids=batch_ids,
            metadatas=[{"source": filename}] * len(batch_texts)
        )
        
        total_added += len(batch_texts)

print(f"âœ… ì„ë² ë”© ì €ì¥ ì™„ë£Œ! ì´ {total_added}ê°œì˜ ë¬¸ë‹¨ì´ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")