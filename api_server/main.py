from fastapi import FastAPI, Request, UploadFile, File, HTTPException, Query
from pydantic import BaseModel
import chromadb
import torch
from transformers import AutoTokenizer, AutoModel
import httpx
import json
from fastapi.middleware.cors import CORSMiddleware
import os
from dotenv import load_dotenv
import gc
from pathlib import Path
import pandas as pd
from fastapi.staticfiles import StaticFiles
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
import asyncio
import time
from datetime import datetime
from utils.logger import get_logger
from utils.session_manager import SessionManager
import shutil
from fastapi.responses import JSONResponse, FileResponse
import sys
import subprocess
import csv
import io
import tempfile
from openpyxl import Workbook
from openpyxl.utils import get_column_letter
import re
import numpy as np
import psutil

# ëª¨ë“ˆë³„ë¡œ ë¡œê±° ìƒì„±
logger = get_logger(__name__)

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ì„¤ì •
ROOT_DIR = Path(__file__).parent.parent
VECTOR_DB_PATH = ROOT_DIR / "vector_db"
DOCS_DIR = ROOT_DIR / "docs"
ENHANCED_FAQ_PATH = DOCS_DIR / "enhanced_qa_pairs.xlsx"
FRONTEND_BUILD_DIR = ROOT_DIR / "frontend" / "build"
LOGS_DIR = ROOT_DIR / "logs"  # ë¡œê·¸ ë””ë ‰í† ë¦¬ ì¶”ê°€
QUESTIONS_DIR = ROOT_DIR / "questions"  # ì§ˆë¬¸ ì €ì¥ ë””ë ‰í† ë¦¬ ì¶”ê°€

# ë””ë ‰í† ë¦¬ ì¡´ì¬ í™•ì¸ ë° ìƒì„±
VECTOR_DB_PATH.mkdir(parents=True, exist_ok=True)
DOCS_DIR.mkdir(parents=True, exist_ok=True)
LOGS_DIR.mkdir(parents=True, exist_ok=True)  # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
QUESTIONS_DIR.mkdir(parents=True, exist_ok=True)  # ì§ˆë¬¸ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±

# ì§ˆë¬¸ ì €ì¥ íŒŒì¼ ê²½ë¡œ
QUESTIONS_FILE = QUESTIONS_DIR / "saved_questions.csv"

load_dotenv()  # .env íŒŒì¼ ì½ê¸°

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ë¶ˆëŸ¬ì˜¤ê¸°
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")

# ëª¨ë¸ ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì •
cache_dir = ROOT_DIR / "model_cache"
cache_dir.mkdir(parents=True, exist_ok=True)

# FastAPI ì•± ìƒì„±
app = FastAPI()

# CORS ì„¤ì • - localhost ìš°ì„ 
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000",  # ë¡œì»¬ ê°œë°œ í™˜ê²½ ìš°ì„ 
        "https://construction-chatbot-api.onrender.com",
        "https://construction-chatbot-frontend.onrender.com",  # í”„ë¡ íŠ¸ì—”ë“œ ë°°í¬ URL ì¶”ê°€
        "*",  # ê°œë°œ ì¤‘ì—ëŠ” ëª¨ë“  ì¶œì²˜ í—ˆìš© (í”„ë¡œë•ì…˜ì—ì„œëŠ” ì œê±°)
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ê°œë°œ í™˜ê²½ ê°ì§€
def is_development_request(request: Request) -> bool:
    """ìš”ì²­ì´ ë¡œì»¬ ê°œë°œ í™˜ê²½ì—ì„œ ì˜¨ ê²ƒì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    origin = request.headers.get("origin", "")
    return origin.startswith("http://localhost:")

# ChromaDB í´ë¼ì´ì–¸íŠ¸ ì„¤ì • - ì „ì—­ ì»¬ë ‰ì…˜ ì´ë¦„ ìƒìˆ˜ ì •ì˜
COLLECTION_NAME = "construction_manuals"  # ì›ë˜ ì»¬ë ‰ì…˜ ì´ë¦„ìœ¼ë¡œ ë˜ëŒë¦¼

# ChromaDB í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
chroma_client = chromadb.PersistentClient(path=str(VECTOR_DB_PATH))
collection = None  # ì‹œì‘ ì‹œì—ëŠ” Noneìœ¼ë¡œ ì„¤ì •, ì´ˆê¸°í™” ê³¼ì •ì—ì„œ ìƒì„±

def get_collection():
    """í˜„ì¬ ì»¬ë ‰ì…˜ì„ ê°€ì ¸ì˜¤ê±°ë‚˜ ì—†ìœ¼ë©´ ìƒì„±í•©ë‹ˆë‹¤."""
    global collection
    try:
        if collection is None:
            try:
                # ê¸°ì¡´ ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸° ì‹œë„
                collection = chroma_client.get_collection(name=COLLECTION_NAME)
                logger.info(f"ê¸°ì¡´ ì»¬ë ‰ì…˜ì„ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤: {COLLECTION_NAME}")
            except Exception as e:
                logger.warning(f"ê¸°ì¡´ ì»¬ë ‰ì…˜ì„ ê°€ì ¸ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
                # ì»¬ë ‰ì…˜ ìƒì„±
                collection = chroma_client.create_collection(name=COLLECTION_NAME)
                logger.info(f"ìƒˆ ì»¬ë ‰ì…˜ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤: {COLLECTION_NAME}")
        
        # ì»¬ë ‰ì…˜ ìœ íš¨ì„± ê²€ì‚¬
        dummy_result = collection.count()
        logger.info(f"ì»¬ë ‰ì…˜ í•­ëª© ìˆ˜: {dummy_result}")
        return collection
    except Exception as e:
        logger.error(f"ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸°/ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        # ë§ˆì§€ë§‰ ì‹œë„: ëª¨ë“  ì»¬ë ‰ì…˜ ì‚­ì œ í›„ ìƒˆë¡œ ìƒì„±
        try:
            chroma_client.delete_collection(name=COLLECTION_NAME)
            collection = chroma_client.create_collection(name=COLLECTION_NAME)
            logger.info(f"ì»¬ë ‰ì…˜ì„ ê°•ì œë¡œ ì¬ìƒì„±í–ˆìŠµë‹ˆë‹¤: {COLLECTION_NAME}")
            return collection
        except Exception as e2:
            logger.error(f"ì»¬ë ‰ì…˜ ê°•ì œ ì¬ìƒì„± ì¤‘ ì˜¤ë¥˜: {e2}")
            raise e2

# FAQ ë°ì´í„° ë¡œë“œ
faq_data = None

def load_enhanced_faq():
    """êµ¬ì¡°í™”ëœ FAQ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    global faq_data
    try:
        if not ENHANCED_FAQ_PATH.exists():
            logger.error(f"Enhanced FAQ file not found at: {ENHANCED_FAQ_PATH}")
            return False
            
        faq_data = pd.read_excel(ENHANCED_FAQ_PATH)
        
        if faq_data.empty:
            logger.error("Enhanced FAQ file is empty")
            return False
        
        # ë°ì´í„° ì»¬ëŸ¼ í™•ì¸ ë¡œê¹…
        logger.info(f"FAQ ë°ì´í„° ì»¬ëŸ¼: {list(faq_data.columns)}")
        logger.info(f"FAQ ë°ì´í„° ì²« í–‰: \n{faq_data.iloc[0]}")
        
        # ë°ì´í„° í˜•ì‹ ê²€ì¦
        for col in ['question_variations', 'structured_answer', 'keywords']:
            if col not in faq_data.columns:
                logger.error(f"Required column '{col}' not found in FAQ data")
                return False
        
        # JSON í•„ë“œ íŒŒì‹± ê²€ì¦
        for idx, row in faq_data.iterrows():
            try:
                for field in ['question_variations', 'structured_answer', 'keywords']:
                    if isinstance(row[field], str):
                        json.loads(row[field])
                    else:
                        logger.warning(f"Row {idx}: {field} is not a string, converting to string")
                        faq_data.at[idx, field] = json.dumps(row[field], ensure_ascii=False)
            except Exception as e:
                logger.error(f"Error parsing JSON in row {idx}: {e}")
                return False
            
        logger.info(f"Enhanced FAQ data loaded successfully: {len(faq_data)} entries found")
        return True
    except Exception as e:
        logger.error(f"Error loading enhanced FAQ: {e}")
        faq_data = None
        return False

# ì‹œì‘ì‹œ FAQ ë°ì´í„° ë¡œë“œ
load_enhanced_faq()

# ëª¨ë¸ ì„¤ì •
model_name = "sentence-transformers/all-MiniLM-L6-v2"
tokenizer = None
model = None

def load_model():
    global tokenizer, model
    if tokenizer is None:
        tokenizer = AutoTokenizer.from_pretrained(model_name, local_files_only=False)
    if model is None:
        model = AutoModel.from_pretrained(
            model_name,
            local_files_only=False
        )
        model.eval()  # í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
    return tokenizer, model

def unload_model():
    global tokenizer, model
    del tokenizer
    del model
    tokenizer = None
    model = None
    torch.cuda.empty_cache() if torch.cuda.is_available() else gc.collect()

# Mean Pooling í•¨ìˆ˜
def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output[0]
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

# í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def get_embeddings(texts, batch_size=32):
    """í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ì§€ì›í•˜ë©° ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´ ìµœëŒ€ ë°°ì¹˜ í¬ê¸°ë¥¼ ì œí•œí•©ë‹ˆë‹¤."""
    if isinstance(texts, str):
        texts = [texts]
    
    # ë¹ˆ ì…ë ¥ í™•ì¸
    if not texts:
        logger.warning("get_embeddings: ì…ë ¥ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        return []
        
    # ë¡œë“œëœ í…ìŠ¤íŠ¸ ê¸¸ì´ ë¡œê¹…
    logger.info(f"get_embeddings: ì²˜ë¦¬í•  í…ìŠ¤íŠ¸ {len(texts)}ê°œ")
    
    # ë°°ì¹˜ê°€ ë„ˆë¬´ í¬ë©´ ë¶„í• í•˜ì—¬ ì¬ê·€ì ìœ¼ë¡œ ì²˜ë¦¬
    if len(texts) > batch_size:
        logger.info(f"ë°°ì¹˜ í¬ê¸°({len(texts)})ê°€ ì œí•œ({batch_size})ì„ ì´ˆê³¼í•˜ì—¬ ë¶„í•  ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        result = []
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            batch_embeddings = get_embeddings(batch_texts)
            result.extend(batch_embeddings)
        return result
    
    try:
        # ëª¨ë¸ ë¡œë“œ
        tokenizer, model = load_model()
        
        # ì¸ì½”ë”©
        encoded_input = tokenizer(
            texts,
            padding=True,
            truncation=True,
            max_length=512,
            return_tensors='pt'
        )
        
        # ì„ë² ë”© ìƒì„±
        with torch.no_grad():
            model_output = model(**encoded_input)
        
        sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])
        sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)
        
        # í…ì„œë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        embeddings_list = sentence_embeddings.tolist()
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        unload_model()
        
        # ì„ë² ë”© í˜•ì‹ í™•ì¸ ë° ë¡œê¹…
        if embeddings_list and len(embeddings_list) > 0:
            logger.info(f"ìƒì„±ëœ ì„ë² ë”© í˜•ì‹: ì°¨ì› ìˆ˜ = {len(embeddings_list)}, ì²« ì„ë² ë”© ê¸¸ì´ = {len(embeddings_list[0])}")
            
            # ì„ë² ë”©ì´ 3ì°¨ì› ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° 2ì°¨ì›ìœ¼ë¡œ ë³€í™˜
            if isinstance(embeddings_list[0], list) and isinstance(embeddings_list[0][0], list):
                logger.info("3ì°¨ì› ì„ë² ë”©ì„ 2ì°¨ì›ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.")
                embeddings_list = [emb[0] for emb in embeddings_list]
            
            # ë‹¨ì¼ ì„ë² ë”©ì¸ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            elif not isinstance(embeddings_list[0], list):
                embeddings_list = [embeddings_list]
                logger.info("ë‹¨ì¼ ì„ë² ë”©ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í–ˆìŠµë‹ˆë‹¤.")
            
            # ìµœì¢… í˜•ì‹ ê²€ì¦
            if not all(isinstance(emb, list) and all(isinstance(x, (int, float)) for x in emb) for emb in embeddings_list):
                logger.error("ì„ë² ë”© í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                return [[0.0] * 384 for _ in range(len(texts))]
        
        return embeddings_list
    except Exception as e:
        logger.error(f"ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ì„ë² ë”© ë°˜í™˜ (ì ì ˆí•œ ì°¨ì›ì˜ 0 ë²¡í„°)
        return [[0.0] * 384 for _ in range(len(texts))]

# ì„¸ì…˜ ë§¤ë‹ˆì € ì´ˆê¸°í™”
session_manager = SessionManager(storage_dir=str(ROOT_DIR / "sessions"))

# ì§ˆë¬¸ ì €ì¥ í•¨ìˆ˜
def save_question(query: str, answer: str = None, similarity: float = None, session_id: str = None):
    """ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ë‹µë³€ì„ CSV íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤."""
    try:
        # íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ í—¤ë”ì™€ í•¨ê»˜ ìƒì„±
        file_exists = QUESTIONS_FILE.exists()
        
        # í˜„ì¬ ì‹œê°„ì„ ì›í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ… (YYYY-MM-DD AM/PM H:MM)
        now = datetime.now()
        formatted_time = now.strftime("%Y-%m-%d %p %I:%M").replace("AM", "ì˜¤ì „").replace("PM", "ì˜¤í›„")
        
        # BOMì„ ì‚¬ìš©í•˜ì—¬ UTF-8ë¡œ ì €ì¥ (Excelì—ì„œ í•œê¸€ ì¸ì½”ë”© ë¬¸ì œ í•´ê²°)
        with open(QUESTIONS_FILE, mode='a', newline='', encoding='utf-8-sig') as file:
            writer = csv.writer(file)
            
            # íŒŒì¼ì´ ìƒˆë¡œ ìƒì„±ëœ ê²½ìš° í—¤ë” ì¶”ê°€
            if not file_exists:
                writer.writerow(['timestamp', 'session_id', 'query', 'answer', 'similarity'])
            
            # ì§ˆë¬¸ê³¼ ë‹µë³€ ì €ì¥
            writer.writerow([
                formatted_time,
                session_id or 'unknown',
                query,
                answer or '',
                f"{similarity:.4f}" if similarity is not None else ''
            ])
        
        logger.info(f"ì§ˆë¬¸ê³¼ ë‹µë³€ ì €ì¥ ì™„ë£Œ: {query}")
        return True
    except Exception as e:
        logger.error(f"ì§ˆë¬¸ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        return False

# ì§ˆë¬¸ í˜•ì‹ ì •ì˜
class Question(BaseModel):
    query: str
    session_id: str = None

# OpenRouter í˜¸ì¶œ í•¨ìˆ˜
def call_openrouter(prompt: str, conversation_history=None) -> str:
    headers = {
        "Authorization": OPENROUTER_API_KEY,
        "Content-Type": "application/json"
    }
    
    # API í‚¤ê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ë©”ì‹œì§€ ë°˜í™˜
    if not OPENROUTER_API_KEY or OPENROUTER_API_KEY == "your_openrouter_api_key_here":
        logger.error("OpenRouter API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return "ì£„ì†¡í•©ë‹ˆë‹¤. OpenRouter API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”."
    
    messages = []
    
    # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì¶”ê°€ (ì¡´ëŒ“ë§ ì§€ì‹œ)
    system_message = {
        "role": "system", 
        "content": "ë‹¹ì‹ ì€ ê±´ì„¤ ì •ë³´ ì‹œìŠ¤í…œì— ëŒ€í•œ ì „ë¬¸ ì§€ì‹ì„ ê°–ì¶˜ ì±—ë´‡ì…ë‹ˆë‹¤. í•­ìƒ ì •ì¤‘í•˜ê³  ê³µì†í•œ ì¡´ëŒ“ë§ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."
    }
    messages.append(system_message)
    
    # ëŒ€í™” ê¸°ë¡ì´ ìˆìœ¼ë©´ ì¶”ê°€
    if conversation_history:
        for msg in conversation_history:
            messages.append({
                "role": msg["role"],
                "content": msg["content"]
            })
    
    # í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    messages.append({"role": "user", "content": prompt})
    
    # ëŒ€í™” ê¸°ë¡ì´ ì—†ëŠ” ê²½ìš°ì—ë„ ì‹œìŠ¤í…œ ë©”ì‹œì§€ëŠ” ìœ ì§€
    if len(messages) <= 2:  # ì‹œìŠ¤í…œ ë©”ì‹œì§€ + í˜„ì¬ ë©”ì‹œì§€ë§Œ ìˆëŠ” ê²½ìš°
        messages = [system_message, {"role": "user", "content": prompt}]
    
    payload = {
        "model": "openai/gpt-3.5-turbo",
        "messages": messages
    }
    
    try:
        logger.info(f"OpenRouter API í˜¸ì¶œ: {prompt[:50]}...")
        
        response = httpx.post(
            url="https://openrouter.ai/api/v1/chat/completions",
            headers=headers,
            json=payload,
            timeout=60.0,
            verify=False  # í…ŒìŠ¤íŠ¸ í™˜ê²½ì—ì„œë§Œ ì‚¬ìš©
        )
        
        # ì‘ë‹µ ìƒíƒœ ì½”ë“œ ë¡œê¹…
        logger.info(f"OpenRouter API ì‘ë‹µ ìƒíƒœ ì½”ë“œ: {response.status_code}")
        
        # ì‘ë‹µ ë‚´ìš© ë””ë²„ê·¸ë¥¼ ìœ„í•´ ë¡œê¹… (ê°œì¸ì •ë³´ëŠ” ì œì™¸)
        logger.info(f"OpenRouter API ì‘ë‹µ í—¤ë”: {dict(response.headers)}")
        
        if response.status_code != 200:
            error_msg = f"OpenRouter API ì˜¤ë¥˜: ìƒíƒœ ì½”ë“œ {response.status_code}"
            try:
                error_data = response.json()
                error_msg += f", ìƒì„¸ ë‚´ìš©: {error_data}"
            except:
                error_msg += f", ì‘ë‹µ: {response.text[:200]}"
            
            logger.error(error_msg)
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. API ì„œë²„ì— ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”. (ì˜¤ë¥˜: {response.status_code})"
        
        data = response.json()
        
        # ì‘ë‹µ ë°ì´í„° êµ¬ì¡° í™•ì¸
        if "choices" not in data or len(data["choices"]) == 0:
            logger.error(f"OpenRouter API ì‘ë‹µì— choices í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤: {data}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. API ì‘ë‹µì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”."
        
        if "message" not in data["choices"][0]:
            logger.error(f"OpenRouter API ì‘ë‹µì— message í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤: {data['choices'][0]}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. API ì‘ë‹µì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”."
        
        return data["choices"][0]["message"]["content"]
    except httpx.HTTPStatusError as e:
        logger.error(f"HTTP ì˜¤ë¥˜: {e.response.status_code} - {e.response.text}")
        return f"ì£„ì†¡í•©ë‹ˆë‹¤. API ì„œë²„ ì‘ë‹µ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. (ìƒíƒœ ì½”ë“œ: {e.response.status_code})"
    except httpx.RequestError as e:
        logger.error(f"ìš”ì²­ ì˜¤ë¥˜: {e}")
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ì„œë²„ ì—°ê²°ì— ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    except Exception as e:
        logger.error(f"OpenRouter API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        return f"ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”. (ì˜¤ë¥˜: {str(e)[:100]})"

def find_faq_match(query: str, threshold: float = 0.75):
    """êµ¬ì¡°í™”ëœ FAQì—ì„œ ê°€ì¥ ì ì ˆí•œ ë‹µë³€ì„ ì°¾ìŠµë‹ˆë‹¤."""
    
    if faq_data is None or faq_data.empty:
        logger.error("FAQ data is not loaded")
        return None, 0.0
    
    try:
        # ì¿¼ë¦¬ ì „ì²˜ë¦¬
        query = query.lower().strip()
        if query.endswith('?') or query.endswith('.'): 
            query = query[:-1]
        
        start_time = time.time()
        
        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        query_embedding = get_embeddings(query)
        
        # ë‹¨ì¼ ì„ë² ë”©ì¸ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ë°°ì¹˜ ì²˜ë¦¬ í•¨ìˆ˜ ë³€ê²½ìœ¼ë¡œ ì¸í•œ ëŒ€ì‘)
        if isinstance(query_embedding, list) and not isinstance(query_embedding[0], list):
            query_embedding = [query_embedding]
            
        # ë§Œì•½ ì„ë² ë”© ê²°ê³¼ê°€ ë¹ˆ ë¦¬ìŠ¤íŠ¸ì´ê±°ë‚˜ Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
        if not query_embedding or len(query_embedding) == 0:
            logger.error("ì„ë² ë”© ìƒì„± ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©")
            # 384ì°¨ì› 0 ë²¡í„° ì‚¬ìš©
            query_embedding = [[0.0] * 384]
        
        # ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸°
        current_collection = get_collection()
        
        logger.info(f"ë²¡í„° ê²€ìƒ‰ ì‹œì‘: {query}")
        
        # ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ëŒì•„ê°€ê¸°: ëª¨ë“  FAQ í•­ëª©ì„ ì§ì ‘ ë¹„êµ
        best_match = None
        highest_similarity = -1
        
        # ì¿¼ë¦¬ì— í¬í•¨ëœ ë‹¨ì–´ ëª©ë¡
        query_words = set(query.split())
        
        # ê° FAQ í•­ëª©ê³¼ ë¹„êµ
        for idx, row in faq_data.iterrows():
            try:
                variations = json.loads(row['question_variations']) if isinstance(row['question_variations'], str) else row['question_variations']
                original_question = row.get('original_question', '')
                
                # keywords í•„ë“œ í™œìš©
                keywords = []
                if 'keywords' in row:
                    try:
                        keywords_data = row['keywords']
                        if isinstance(keywords_data, str):
                            keywords = json.loads(keywords_data)
                        else:
                            keywords = keywords_data
                    except Exception as e:
                        logger.error(f"í‚¤ì›Œë“œ íŒŒì‹± ì˜¤ë¥˜: {e}")
                
                logger.info(f"FAQ í•­ëª© {idx + 1}/{len(faq_data)}: ì›ë³¸ ì§ˆë¬¸: '{original_question}', í‚¤ì›Œë“œ: {keywords}")
                
                # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
                keyword_bonus = 0.0
                matching_keywords = []
                
                for keyword in keywords:
                    # ì •í™•í•œ í‚¤ì›Œë“œ ë§¤ì¹­ í™•ì¸ (ë¶€ë¶„ ë§¤ì¹­ì´ ì•„ë‹Œ ì™„ì „ ì¼ì¹˜)
                    # 1. ì¿¼ë¦¬ì— í‚¤ì›Œë“œê°€ ì •í™•íˆ í¬í•¨ë˜ëŠ”ì§€ í™•ì¸ (ë‹¨ì–´ ê²½ê³„ ê³ ë ¤)
                    exact_match = False
                    
                    # í‚¤ì›Œë“œë¥¼ ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ê³  ê³µë°± ì²˜ë¦¬
                    keyword_lower = keyword.lower().strip()
                    # ë„ì–´ì“°ê¸°ë¥¼ ì œê±°í•œ ë²„ì „ë„ ìƒì„±
                    keyword_no_space = keyword_lower.replace(" ", "")
                    
                    # ì¿¼ë¦¬ì—ì„œ ë„ì–´ì“°ê¸° ì œê±°í•œ ë²„ì „
                    query_no_space = query.lower().replace(" ", "")
                    # ì¿¼ë¦¬ ë‹¨ì–´ë¥¼ ëª¨ë‘ ë¶™ì¸ ë²„ì „
                    query_words_joined = "".join(query_words)
                    
                    # ë°©ë²• 1: ì¿¼ë¦¬ì—ì„œ í‚¤ì›Œë“œë¥¼ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ì°¾ê¸°
                    if keyword_lower in query_words:
                        exact_match = True
                        logger.info(f"ë‹¨ì–´ ë‹¨ìœ„ ë§¤ì¹˜: '{keyword}'")
                    
                    # ë°©ë²• 2: ì¿¼ë¦¬ì—ì„œ í‚¤ì›Œë“œë¥¼ ë‹¨ì–´ ê²½ê³„ë¥¼ ê³ ë ¤í•˜ì—¬ ì°¾ê¸°
                    elif re.search(r'\b' + re.escape(keyword_lower) + r'\b', query.lower()):
                        exact_match = True
                        logger.info(f"ë‹¨ì–´ ê²½ê³„ ë§¤ì¹˜: '{keyword}'")
                    
                    # ë°©ë²• 3: ì¿¼ë¦¬ê°€ ê¸°ë³¸ì ìœ¼ë¡œ ì§§ì€ ê²½ìš° ì •í™•í•œ í‚¤ì›Œë“œë§Œ ì²´í¬
                    elif len(query_words) <= 3 and query.lower() == keyword_lower:
                        exact_match = True
                        logger.info(f"ì „ì²´ í…ìŠ¤íŠ¸ ë§¤ì¹˜: '{keyword}'")
                    
                    # ë°©ë²• 4: ë„ì–´ì“°ê¸°ë¥¼ ë¬´ì‹œí•˜ê³  ë§¤ì¹­
                    elif keyword_no_space == query_no_space:
                        exact_match = True
                        logger.info(f"ë„ì–´ì“°ê¸° ë¬´ì‹œ ì „ì²´ ë§¤ì¹˜: '{keyword}' vs '{query}'")
                    
                    # ë°©ë²• 5: ë„ì–´ì“°ê¸°ë¥¼ ë¬´ì‹œí•œ ë¶€ë¶„ ë¬¸ìì—´ ë§¤ì¹­ (í‚¤ì›Œë“œê°€ ê½¤ ê¸¸ ê²½ìš°ì—ë§Œ)
                    elif len(keyword_no_space) >= 4 and keyword_no_space in query_no_space:
                        # í‚¤ì›Œë“œê°€ ì¶©ë¶„íˆ ê¸¸ ê²½ìš°ì—ë§Œ ë¶€ë¶„ ë¬¸ìì—´ ë§¤ì¹­ í—ˆìš©
                        exact_match = True
                        logger.info(f"ë„ì–´ì“°ê¸° ë¬´ì‹œ ë¶€ë¶„ ë§¤ì¹˜: '{keyword}' in '{query}'")
                    
                    if exact_match:
                        matching_keywords.append(keyword)
                        keyword_bonus += 0  # í‚¤ì›Œë“œë‹¹ 0.3ì  ë³´ë„ˆìŠ¤
                        logger.info(f"ì •í™•í•œ í‚¤ì›Œë“œ ë§¤ì¹˜: '{keyword}'")
                
                if matching_keywords:
                    logger.info(f"í‚¤ì›Œë“œ ë§¤ì¹˜ ë°œê²¬: {matching_keywords}, ë³´ë„ˆìŠ¤: {keyword_bonus}")
                
                for q in variations:
                    # ì§ˆë¬¸ ì „ì²˜ë¦¬
                    q = q.lower().strip()
                    if q.endswith('?') or q.endswith('.'): 
                        q = q[:-1]
                        
                    # ì§ì ‘ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                    try:
                        # qì— ëŒ€í•œ ì„ë² ë”© ì¡°íšŒ
                        q_embedding = get_embeddings(q)
                        
                        # ë‹¨ì¼ ì„ë² ë”©ì¸ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                        if isinstance(q_embedding, list) and not isinstance(q_embedding[0], list):
                            q_embedding = [q_embedding]
                        
                        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                        similarity = torch.cosine_similarity(
                            torch.tensor(query_embedding[0]),
                            torch.tensor(q_embedding[0]),
                            dim=0
                        ).item()
                        
                        # í‚¤ì›Œë“œ ë³´ë„ˆìŠ¤ ì ìš©
                        adjusted_similarity = similarity + keyword_bonus
                        
                        logger.info(f"ì§ˆë¬¸: '{q}', ê¸°ë³¸ ìœ ì‚¬ë„: {similarity:.4f}, í‚¤ì›Œë“œ ë³´ë„ˆìŠ¤: {keyword_bonus:.2f}, ìµœì¢…: {adjusted_similarity:.4f}")
                        
                        if adjusted_similarity > highest_similarity:
                            highest_similarity = adjusted_similarity
                            best_match = row
                            logger.info(f"ìƒˆ ìµœê³  ë§¤ì¹˜: '{original_question}', ìœ ì‚¬ë„: {adjusted_similarity:.4f}, ë§¤ì¹­ í‚¤ì›Œë“œ: {matching_keywords}")
                    except Exception as e:
                        logger.error(f"ìœ ì‚¬ë„ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
                        continue
            except Exception as e:
                logger.error(f"ë³€í˜• ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        end_time = time.time()
        logger.info(f"ê²€ìƒ‰ ì™„ë£Œ: ì†Œìš” ì‹œê°„ = {end_time - start_time:.2f}ì´ˆ, ìµœê³  ìœ ì‚¬ë„ = {highest_similarity:.4f}")
        
        # ìµœì¢… ìœ ì‚¬ë„ê°€ í‚¤ì›Œë“œ ë³´ë„ˆìŠ¤ ë•Œë¬¸ì— ì„ê³„ê°’ì„ ë„˜ì—ˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, 
        # ì›ë˜ ì„ê³„ê°’ë³´ë‹¤ í¬ê±°ë‚˜ ê°™ì€ì§€ í™•ì¸
        if highest_similarity >= threshold and best_match is not None:
            return best_match, highest_similarity
        return None, highest_similarity if highest_similarity > -1 else 0.0
    except Exception as e:
        logger.error(f"Error in find_faq_match: {e}")
        import traceback
        logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        return None, 0.0

@app.post("/ask")
async def ask_question(q: Question, request: Request):
    """ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ê³  ë‹µë³€ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        logger.info(f"Received question: {q.query}, session_id: {q.session_id}")
        start_time = time.time()
        
        # ê°œë°œ í™˜ê²½ ì—¬ë¶€ í™•ì¸
        is_dev = is_development_request(request)
        
        # ì„¸ì…˜ ì²˜ë¦¬
        if not q.session_id:
            session = session_manager.create_session()
            q.session_id = session.session_id
            logger.info(f"Created new session: {q.session_id}")
        else:
            # ì„¸ì…˜ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            session = session_manager.get_session(q.session_id)
            if not session:
                # ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì„¸ì…˜ IDì¸ ê²½ìš° ìƒˆ ì„¸ì…˜ ìƒì„±
                session = session_manager.create_session()
                q.session_id = session.session_id
                logger.info(f"Invalid session ID provided, created new: {q.session_id}")
        
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥
        session_manager.add_message(q.session_id, "user", q.query)
        
        # FAQ ë§¤ì¹­ ì‹œë„
        try:
            faq_match_start = time.time()
            faq_match, similarity = find_faq_match(q.query)
            faq_match_time = time.time() - faq_match_start
            
            logger.info(f"FAQ ë§¤ì¹­ ì†Œìš” ì‹œê°„: {faq_match_time:.2f}ì´ˆ")
            
            if faq_match is not None:
                logger.info(f"FAQ match found for query: {q.query}")
                
                # faq_match ê°ì²´ì˜ êµ¬ì¡° í™•ì¸
                logger.info(f"FAQ match columns: {list(faq_match.index)}")
                
                # ì‘ë‹µ ë°ì´í„° ìƒì„±
                response_data = {
                    "session_id": q.session_id,
                    "sources": [faq_match.get("source", "FAQ")],
                    "is_dev": is_dev,
                    "type": "faq",
                    "debug_info": {
                        "source": "faq",
                        "confidence": similarity,
                        "match_time": f"{faq_match_time:.2f}ì´ˆ"
                    }
                }
                
                # ì›ë³¸ ì§ˆë¬¸ ì¶”ê°€
                if 'original_question' in faq_match:
                    response_data["original_question"] = faq_match["original_question"]
                
                # ë‹µë³€ ë°ì´í„° ì¶”ê°€
                if 'answer' in faq_match:
                    # ë‹¨ìˆœ í…ìŠ¤íŠ¸ ë‹µë³€
                    formatted_answer = faq_match["answer"]
                    response_data["answer"] = formatted_answer
                    session_manager.add_message(q.session_id, "assistant", formatted_answer)
                    
                    # ì§ˆë¬¸ê³¼ ë‹µë³€ ì €ì¥
                    save_question(q.query, formatted_answer, similarity, q.session_id)
                    
                elif 'structured_answer' in faq_match:
                    # êµ¬ì¡°í™”ëœ ë‹µë³€ - JSON ê°ì²´ ê·¸ëŒ€ë¡œ ì „ë‹¬
                    structured_answer = faq_match["structured_answer"]
                    if isinstance(structured_answer, str):
                        structured_answer = json.loads(structured_answer)
                    
                    # êµ¬ì¡°í™”ëœ ë°ì´í„° ì „ë‹¬
                    response_data["structured_answer"] = structured_answer
                    
                    # êµ¬ì¡°í™”ëœ ë‹µë³€ì„ ì½ê¸° ì‰¬ìš´ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ ì„¸ì…˜ì— ì €ì¥
                    readable_text = ""
                    
                    # ì›ë³¸ ì§ˆë¬¸ ì¶”ê°€
                    if "original_question" in faq_match:
                        readable_text += f"ì§ˆë¬¸: {faq_match['original_question']}\n\n"
                    
                    # ê¸°ë³¸ ê·œì¹™ ì¶”ê°€
                    if "basic_rules" in structured_answer and structured_answer["basic_rules"]:
                        readable_text += "â€¢ ê¸°ë³¸ ê·œì¹™:\n"
                        for rule in structured_answer["basic_rules"]:
                            readable_text += f"  - {rule}\n"
                        readable_text += "\n"
                    
                    # ì˜ˆì‹œ ì¶”ê°€
                    if "examples" in structured_answer and structured_answer["examples"]:
                        readable_text += "â€¢ ì˜ˆì‹œ:\n"
                        for example in structured_answer["examples"]:
                            if isinstance(example, dict):
                                # ë”•ì…”ë„ˆë¦¬ í˜•íƒœì˜ ì˜ˆì‹œëŠ” ì‹œë‚˜ë¦¬ì˜¤ì™€ ê²°ê³¼ë¡œ êµ¬ë¶„í•˜ì—¬ ì²˜ë¦¬
                                for key, value in example.items():
                                    if key.lower().startswith('scenario'):
                                        readable_text += f"  ğŸ“Œ {value}\n"
                                    elif key.lower().startswith('result'):
                                        readable_text += f"      â¡ï¸ {value}\n"
                                    else:
                                        readable_text += f"      â€¢ {key}: {value}\n"
                            else:
                                readable_text += f"  - {example}\n"
                        readable_text += "\n"
                    
                    # ì£¼ì˜ì‚¬í•­ ì¶”ê°€
                    if "cautions" in structured_answer and structured_answer["cautions"]:
                        readable_text += "â€¢ ì£¼ì˜ì‚¬í•­:\n"
                        for caution in structured_answer["cautions"]:
                            readable_text += f"  - {caution}\n"
                        readable_text += "\n"
                    
                    # ì„¸ì…˜ì— ì €ì¥
                    session_manager.add_message(q.session_id, "assistant", readable_text)
                    
                    # êµ¬ì¡°í™”ëœ ë‹µë³€ì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
                    answer_text = json.dumps(structured_answer, ensure_ascii=False)
                    save_question(q.query, answer_text, similarity, q.session_id)
                    
                else:
                    # ë‹µë³€ ì—†ìŒ
                    formatted_answer = "ì£„ì†¡í•©ë‹ˆë‹¤. ì´ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                    response_data["answer"] = formatted_answer
                    session_manager.add_message(q.session_id, "assistant", formatted_answer)
                    
                    # ì§ˆë¬¸ê³¼ ë‹µë³€ ì €ì¥
                    save_question(q.query, formatted_answer, similarity, q.session_id)
                    
                    logger.error(f"FAQ match found but no answer field: {faq_match}")
                
                # ì´ ì²˜ë¦¬ ì‹œê°„ ì¸¡ì •
                total_time = time.time() - start_time
                response_data["debug_info"]["total_time"] = f"{total_time:.2f}ì´ˆ"
                
                return response_data
            
            # OpenRouter API í˜¸ì¶œ
            openrouter_start = time.time()
            logger.info(f"No FAQ match found, calling OpenRouter for query: {q.query}")
            answer = call_openrouter(q.query, session_manager.get_messages(q.session_id))
            openrouter_time = time.time() - openrouter_start
            session_manager.add_message(q.session_id, "assistant", answer)
            
            # ì§ˆë¬¸ê³¼ ë‹µë³€ ì €ì¥ (OpenRouterì˜ ê²½ìš° similarityëŠ” 0ìœ¼ë¡œ ì €ì¥)
            save_question(q.query, answer, 0.0, q.session_id)
            
            # ì´ ì²˜ë¦¬ ì‹œê°„ ì¸¡ì •
            total_time = time.time() - start_time
            
            return {
                "answer": answer,
                "session_id": q.session_id,
                "sources": [],
                "type": "openrouter",
                "is_dev": is_dev,
                "debug_info": {
                    "source": "openrouter",
                    "similarity": 0.0,
                    "faq_match_time": f"{faq_match_time:.2f}ì´ˆ",
                    "openrouter_time": f"{openrouter_time:.2f}ì´ˆ",
                    "total_time": f"{total_time:.2f}ì´ˆ"
                }
            }
        except Exception as e:
            logger.error(f"FAQ ë§¤ì¹­ ì¤‘ ì˜¤ë¥˜: {e}")
            # OpenRouterë¡œ í´ë°±
            openrouter_start = time.time()
            logger.info(f"FAQ ë§¤ì¹­ ì‹¤íŒ¨, OpenRouter í˜¸ì¶œ: {q.query}")
            answer = call_openrouter(q.query, session_manager.get_messages(q.session_id))
            openrouter_time = time.time() - openrouter_start
            session_manager.add_message(q.session_id, "assistant", answer)
            
            # ì§ˆë¬¸ê³¼ ë‹µë³€ ì €ì¥ (OpenRouterì˜ ê²½ìš° similarityëŠ” 0ìœ¼ë¡œ ì €ì¥)
            save_question(q.query, answer, 0.0, q.session_id)
            
            # ì´ ì²˜ë¦¬ ì‹œê°„ ì¸¡ì •
            total_time = time.time() - start_time
            
            return {
                "answer": answer,
                "session_id": q.session_id,
                "sources": [],
                "type": "openrouter",
                "is_dev": is_dev,
                "debug_info": {
                    "source": "openrouter",
                    "error": str(e),
                    "faq_match_time": f"{faq_match_time:.2f}ì´ˆ",
                    "openrouter_time": f"{openrouter_time:.2f}ì´ˆ",
                    "total_time": f"{total_time:.2f}ì´ˆ"
                }
            }
    except Exception as e:
        logger.error(f"Error processing question: {e}")
        import traceback
        logger.error(f"Traceback: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/session/{session_id}")
async def get_session(session_id: str):
    """ì„¸ì…˜ì˜ ëŒ€í™” ê¸°ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    messages = session_manager.get_messages(session_id)
    if not messages:
        raise HTTPException(status_code=404, detail="Session not found")
    return {"messages": messages}

@app.post("/session/cleanup")
async def cleanup_sessions():
    """ì˜¤ë˜ëœ ì„¸ì…˜ì„ ì •ë¦¬í•©ë‹ˆë‹¤."""
    session_manager.cleanup_old_sessions()
    return {"status": "success", "message": "Sessions cleaned up"}

@app.get("/health")
async def health_check():
    return {"status": "ok"}

@app.get("/reload-faq")
async def reload_faq():
    """FAQ ë°ì´í„°ë¥¼ ì¬ë¡œë“œí•©ë‹ˆë‹¤."""
    load_enhanced_faq()
    return {"status": "FAQ reloaded"}

# ì—¬ê¸°ì—ì„œ initialize_vector_db í•¨ìˆ˜ ìˆ˜ì •
async def initialize_vector_db(force_rebuild=False):
    """êµ¬ì¡°í™”ëœ FAQ ë°ì´í„°ë¥¼ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì„ë² ë”©í•©ë‹ˆë‹¤."""
    global collection
    try:
        # ê¸°ì¡´ ì»¬ë ‰ì…˜ì´ ìˆëŠ”ì§€ í™•ì¸
        existing_collection = False
        try:
            test_collection = chroma_client.get_collection(name=COLLECTION_NAME)
            collection_count = test_collection.count()
            if collection_count > 0:
                existing_collection = True
                logger.info(f"ê¸°ì¡´ ì»¬ë ‰ì…˜ì´ ì¡´ì¬í•¨: {COLLECTION_NAME}, í•­ëª© ìˆ˜: {collection_count}")
        except Exception as e:
            logger.warning(f"ê¸°ì¡´ ì»¬ë ‰ì…˜ í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            existing_collection = False
        
        # ê¸°ì¡´ ì»¬ë ‰ì…˜ì´ ìˆê³  ê°•ì œ ì¬êµ¬ì¶•ì´ ì•„ë‹ˆë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        if existing_collection and not force_rebuild:
            collection = test_collection
            logger.info(f"ê¸°ì¡´ ì»¬ë ‰ì…˜ì„ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤: {COLLECTION_NAME}")
            return {"success": True, "message": f"ê¸°ì¡´ ì»¬ë ‰ì…˜ ì¬ì‚¬ìš© ({collection_count}ê°œ í•­ëª©)"}
        
        # ì•„ë˜ëŠ” ê¸°ì¡´ ì„ë² ë”© ìƒì„± ì½”ë“œ (ìƒˆë¡œ ìƒì„±í•´ì•¼ í•˜ëŠ” ê²½ìš°)
        if not load_enhanced_faq():
            logger.error("êµ¬ì¡°í™”ëœ FAQ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return {"success": False, "message": "êµ¬ì¡°í™”ëœ FAQ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
        
        if faq_data is None or faq_data.empty:
            logger.error("FAQ ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
            return {"success": False, "message": "FAQ ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤."}
        
        logger.info("ìƒˆ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
        
        # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ í›„ ì¬ìƒì„±
        try:
            chroma_client.delete_collection(name=COLLECTION_NAME)
            logger.info(f"ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ ì™„ë£Œ: {COLLECTION_NAME}")
        except Exception as e:
            logger.warning(f"ì»¬ë ‰ì…˜ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ë¬´ì‹œë¨): {e}")
            
        # ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±
        collection = chroma_client.create_collection(name=COLLECTION_NAME)
        logger.info(f"ìƒˆ ì»¬ë ‰ì…˜ì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë¨: {COLLECTION_NAME}")
        
        texts = []
        metadatas = []
        ids = []
        
        # ê° FAQ í•­ëª©ì— ëŒ€í•´ ì„ë² ë”© ìƒì„±
        total_items = len(faq_data)
        logger.info(f"ì´ ì²˜ë¦¬í•  FAQ í•­ëª©: {total_items}ê°œ")
        
        for idx, row in faq_data.iterrows():
            # ì§„í–‰ë¥  í‘œì‹œ
            progress_percent = (idx + 1) / total_items * 100
            logger.info(f"FAQ ì„ë² ë”© ì§„í–‰ë¥ : {progress_percent:.1f}% ({idx + 1}/{total_items})")
            
            # JSON ë¬¸ìì—´ì„ íŒŒì‹±
            question_variations = json.loads(row['question_variations']) if isinstance(row['question_variations'], str) else row['question_variations']
            structured_answer = json.loads(row['structured_answer']) if isinstance(row['structured_answer'], str) else row['structured_answer']
            keywords = json.loads(row['keywords']) if isinstance(row['keywords'], str) else row['keywords']
            original_question = row['original_question']
            
            logger.info(f"Processing FAQ item {idx + 1}/{total_items}: {original_question}")
            
            # ì›ë³¸ ì§ˆë¬¸ ì„ë² ë”©
            texts.append(original_question)
            metadatas.append({
                "type": "original_question",
                "structured_answer": json.dumps(structured_answer, ensure_ascii=False),
                "keywords": json.dumps(keywords, ensure_ascii=False)
            })
            ids.append(f"orig_{idx}")
            
            # ì§ˆë¬¸ ë³€í˜•ë“¤ ì„ë² ë”©
            for var_idx, q in enumerate(question_variations):
                texts.append(q)
                metadatas.append({
                    "type": "question_variation",
                    "original_question": original_question,
                    "structured_answer": json.dumps(structured_answer, ensure_ascii=False),
                    "keywords": json.dumps(keywords, ensure_ascii=False)
                })
                ids.append(f"q_{idx}_{var_idx}")
            
            # êµ¬ì¡°í™”ëœ ë‹µë³€ì˜ ê° ë¶€ë¶„ë„ ì„ë² ë”©
            if 'basic_rules' in structured_answer and structured_answer['basic_rules']:
                for part_idx, rule in enumerate(structured_answer['basic_rules']):
                    text = f"ê¸°ë³¸ ê·œì¹™: {rule}"
                    texts.append(text)
                    metadatas.append({
                        "type": "answer_part",
                        "part_type": "basic_rule",
                        "original_question": original_question,
                        "structured_answer": json.dumps(structured_answer, ensure_ascii=False),
                        "keywords": json.dumps(keywords, ensure_ascii=False)
                    })
                    ids.append(f"rule_{idx}_{part_idx}")
            
            if 'examples' in structured_answer and structured_answer['examples']:
                for part_idx, example in enumerate(structured_answer['examples']):
                    if isinstance(example, dict):
                        example_text = json.dumps(example, ensure_ascii=False)
                    else:
                        example_text = str(example)
                    
                    text = f"ì˜ˆì‹œ: {example_text}"
                    texts.append(text)
                    metadatas.append({
                        "type": "answer_part",
                        "part_type": "example",
                        "original_question": original_question,
                        "structured_answer": json.dumps(structured_answer, ensure_ascii=False),
                        "keywords": json.dumps(keywords, ensure_ascii=False)
                    })
                    ids.append(f"example_{idx}_{part_idx}")
            
            if 'cautions' in structured_answer and structured_answer['cautions']:
                for part_idx, caution in enumerate(structured_answer['cautions']):
                    text = f"ì£¼ì˜ì‚¬í•­: {caution}"
                    texts.append(text)
                    metadatas.append({
                        "type": "answer_part",
                        "part_type": "caution",
                        "original_question": original_question,
                        "structured_answer": json.dumps(structured_answer, ensure_ascii=False),
                        "keywords": json.dumps(keywords, ensure_ascii=False)
                    })
                    ids.append(f"caution_{idx}_{part_idx}")
        
        logger.info(f"ìƒì„±í•  ì„ë² ë”© í•­ëª© ìˆ˜: {len(texts)}")
        
        # ì„ë² ë”© ìƒì„± ë° ì €ì¥
        if texts:
            logger.info("ì„ë² ë”© ìƒì„± ì‹œì‘...")
            
            # ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
            batch_size = 10  # 50ì—ì„œ 10ìœ¼ë¡œ ê°ì†Œ
            
            # embeddings ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
            embeddings = []
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”
            for i in range(0, len(texts), batch_size):
                batch_end = min(i + batch_size, len(texts))
                batch = texts[i:batch_end]
                
                logger.info(f"ì„ë² ë”© ìƒì„± ì§„í–‰ë¥ : {batch_end / len(texts) * 100:.1f}% ({batch_end}/{len(texts)})")
                
                # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì„ë² ë”© ìƒì„±
                batch_embeddings = get_embeddings(batch)
                
                # ì„ë² ë”© í˜•ì‹ í™•ì¸ ë° ìˆ˜ì •
                if batch_embeddings:
                    # ë§Œì•½ 3ì°¨ì› ë¦¬ìŠ¤íŠ¸ë¼ë©´ 2ì°¨ì›ìœ¼ë¡œ ë³€í™˜
                    while isinstance(batch_embeddings, list) and len(batch_embeddings) > 0 and isinstance(batch_embeddings[0], list) and isinstance(batch_embeddings[0][0], list):
                        batch_embeddings = [emb for sublist in batch_embeddings for emb in sublist]
                        logger.info("3ì°¨ì› ì„ë² ë”©ì„ 2ì°¨ì›ìœ¼ë¡œ ë³€í™˜í–ˆìŠµë‹ˆë‹¤.")

                    # ë§Œì•½ 1ì°¨ì› ë¦¬ìŠ¤íŠ¸(ë‹¨ì¼ ë²¡í„°)ë¼ë©´ 2ì°¨ì›ìœ¼ë¡œ ë³€í™˜
                    if batch_embeddings and isinstance(batch_embeddings[0], (float, int)):
                        batch_embeddings = [batch_embeddings]
                        logger.info("ë‹¨ì¼ ì„ë² ë”©ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í–ˆìŠµë‹ˆë‹¤.")

                    embeddings.extend(batch_embeddings)
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                gc.collect()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                
                # ì ì‹œ ëŒ€ê¸°í•˜ì—¬ ë©”ëª¨ë¦¬ í•´ì œ ì‹œê°„ í™•ë³´
                await asyncio.sleep(0.1)
            
            logger.info("ì„ë² ë”© ìƒì„± ì™„ë£Œ! ì´ì œ ChromaDBì— ì €ì¥í•©ë‹ˆë‹¤...")
            
            # ì„ë² ë”© í˜•ì‹ ë¡œê¹… (ë””ë²„ê¹…ìš©)
            if embeddings:
                logger.info(f"ì²« ë²ˆì§¸ ì„ë² ë”© ì°¨ì› êµ¬ì¡°: {type(embeddings)}, {type(embeddings[0])}")
                if isinstance(embeddings[0], list):
                    logger.info(f"ì²« ë²ˆì§¸ ì„ë² ë”© ê¸¸ì´: {len(embeddings[0])}")
            
            # ìµœì¢… ì„ë² ë”© í˜•ì‹ ê²€ì¦
            if not all(isinstance(emb, list) and all(isinstance(x, (int, float)) for x in emb) for emb in embeddings):
                logger.error("ì„ë² ë”© í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                return {"success": False, "message": "ì„ë² ë”© í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤."}
            
            # === ì„ë² ë”© ì°¨ì› ê°•ì œ ë³€í™˜ (ìµœì¢… ë°©ì–´) ===
            if isinstance(embeddings, np.ndarray):
                embeddings = embeddings.tolist()
            while isinstance(embeddings, list) and len(embeddings) > 0 and isinstance(embeddings[0], list) and isinstance(embeddings[0][0], list):
                embeddings = [emb for sublist in embeddings for emb in sublist]
            if embeddings and isinstance(embeddings[0], (float, int)):
                embeddings = [embeddings]
            # === ë°©ì–´ ë ===

            # ì»¬ë ‰ì…˜ì— ì¶”ê°€
            collection.add(
                embeddings=embeddings,
                documents=texts,
                metadatas=metadatas,
                ids=ids
            )
            
            logger.info(f"ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— {len(texts)}ê°œ í•­ëª© ì¶”ê°€ ì™„ë£Œ")
            
            # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì‹¤í–‰
            test_emb = get_embeddings("í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬")
            while isinstance(test_emb, list) and len(test_emb) > 0 and isinstance(test_emb[0], list) and isinstance(test_emb[0][0], list):
                test_emb = [emb for sublist in test_emb for emb in sublist]
            if test_emb and isinstance(test_emb[0], (float, int)):
                test_emb = [test_emb]
            test_result = collection.query(
                query_embeddings=test_emb,
                n_results=1
            )
            logger.info(f"ë²¡í„° DB í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ê²°ê³¼: {test_result}")
            
            return {"success": True, "message": f"{len(texts)}ê°œ í•­ëª©ì´ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤."}
        else:
            logger.error("ì„ë² ë”©í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {"success": False, "message": "ì„ë² ë”©í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."}
            
    except Exception as e:
        logger.error(f"ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return {"success": False, "message": f"ì˜¤ë¥˜: {str(e)}"}

# íŒŒì¼ ë³€ê²½ ê°ì§€ë¥¼ ìœ„í•œ ì „ì—­ ë³€ìˆ˜
last_modified_time = None
file_check_interval = 5  # 5ì´ˆë§ˆë‹¤ í™•ì¸

class ExcelFileHandler(FileSystemEventHandler):
    def __init__(self, callback):
        self.callback = callback
        self.last_modified = 0
        
    def on_modified(self, event):
        if not event.is_directory and event.src_path.endswith('qa_pairs.xlsx'):
            # ì¤‘ë³µ ì´ë²¤íŠ¸ ë°©ì§€ë¥¼ ìœ„í•œ ì¿¨ë‹¤ìš´ ì²´í¬
            current_time = time.time()
            if current_time - self.last_modified > 1:  # 1ì´ˆ ì¿¨ë‹¤ìš´
                self.last_modified = current_time
                asyncio.create_task(self.callback())

async def check_file_changes():
    """qa_pairs.xlsx íŒŒì¼ì˜ ë³€ê²½ì„ ì£¼ê¸°ì ìœ¼ë¡œ í™•ì¸í•©ë‹ˆë‹¤."""
    global last_modified_time
    
    while True:
        try:
            if ENHANCED_FAQ_PATH.exists():
                current_mtime = os.path.getmtime(ENHANCED_FAQ_PATH)
                if last_modified_time is None:
                    last_modified_time = current_mtime
                elif current_mtime > last_modified_time:
                    logger.info("enhanced_qa_pairs.xlsx íŒŒì¼ì´ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤. ë²¡í„° DBë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.")
                    last_modified_time = current_mtime
                    # íŒŒì¼ ë³€ê²½ ì‹œì—ëŠ” force_rebuild=Trueë¡œ ì„¤ì •í•˜ì—¬ ì„ë² ë”© ë‹¤ì‹œ ìƒì„±
                    await initialize_vector_db(force_rebuild=True)
            await asyncio.sleep(file_check_interval)
        except Exception as e:
            logger.error(f"íŒŒì¼ ë³€ê²½ í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            await asyncio.sleep(file_check_interval)

# ë§ˆì§€ë§‰ìœ¼ë¡œ ì •ì  íŒŒì¼ ì„œë¹™ ì„¤ì •í•˜ê¸° ì „ì— ì¶”ê°€
# Ping ì—”ë“œí¬ì¸íŠ¸ - ì„œë²„ í™œì„± ìƒíƒœ ìœ ì§€ìš©
@app.get("/ping")
async def ping():
    """ì„œë²„ê°€ í™œì„± ìƒíƒœì„ì„ í™•ì¸í•˜ëŠ” ë‹¨ìˆœ ì—”ë“œí¬ì¸íŠ¸"""
    return {"status": "alive", "timestamp": datetime.now().isoformat()}

# ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘ í•¨ìˆ˜
def collect_system_info():
    """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ì„ ìˆ˜ì§‘í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        # í”„ë¡œì„¸ìŠ¤ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        process = psutil.Process()
        
        # ë©”ëª¨ë¦¬ ì •ë³´
        memory_info = process.memory_info()
        memory_percent = process.memory_percent()
        
        # ì‹œìŠ¤í…œ ì „ì²´ ë©”ëª¨ë¦¬ ì •ë³´
        system_memory = psutil.virtual_memory()
        
        # CPU ì‚¬ìš©ëŸ‰
        cpu_percent = process.cpu_percent(interval=1)
        
        # ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰
        disk_usage = psutil.disk_usage('/')
        
        info = {
            "process_memory": {
                "rss": f"{memory_info.rss / 1024 / 1024:.2f} MB",  # ì‹¤ì œ ë¬¼ë¦¬ì  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
                "vms": f"{memory_info.vms / 1024 / 1024:.2f} MB",  # ê°€ìƒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
                "percent": f"{memory_percent:.2f}%"
            },
            "system_memory": {
                "total": f"{system_memory.total / 1024 / 1024:.2f} MB",
                "available": f"{system_memory.available / 1024 / 1024:.2f} MB",
                "used": f"{system_memory.used / 1024 / 1024:.2f} MB",
                "percent": f"{system_memory.percent}%"
            },
            "cpu": {
                "percent": f"{cpu_percent}%"
            },
            "disk": {
                "total": f"{disk_usage.total / 1024 / 1024:.2f} MB",
                "used": f"{disk_usage.used / 1024 / 1024:.2f} MB",
                "free": f"{disk_usage.free / 1024 / 1024:.2f} MB",
                "percent": f"{disk_usage.percent}%"
            }
        }
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ 80%ë¥¼ ì´ˆê³¼í•˜ë©´ ê²½ê³  ë¡œê·¸
        if system_memory.percent > 80:
            logger.warning(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìŠµë‹ˆë‹¤! ({system_memory.percent}%)")
        
        return info
    except Exception as e:
        logger.error(f"ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {"error": str(e)}

@app.get("/system-info")
async def get_system_info():
    """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return collect_system_info()

# ìë™ í•‘ì„ ìœ„í•œ ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬
async def keep_alive():
    """ì„œë²„ê°€ ìŠ¬ë¦½ ìƒíƒœë¡œ ì „í™˜ë˜ì§€ ì•Šë„ë¡ ì£¼ê¸°ì ìœ¼ë¡œ ìì²´ í•‘ì„ ìˆ˜í–‰í•˜ê³  ì‹œìŠ¤í…œ ì •ë³´ë¥¼ ëª¨ë‹ˆí„°ë§"""
    while True:
        try:
            # 5ë¶„ë§ˆë‹¤ ìì²´ ping (Render ë¬´ë£Œ í”Œëœ íƒ€ì„ì•„ì›ƒì€ ì¼ë°˜ì ìœ¼ë¡œ 15ë¶„)
            await asyncio.sleep(300)
            
            # ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘ ë° ë¡œê¹…
            system_info = collect_system_info()
            logger.info("ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰:")
            logger.info(f"- í”„ë¡œì„¸ìŠ¤ ë©”ëª¨ë¦¬ (RSS): {system_info['process_memory']['rss']}")
            logger.info(f"- ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {system_info['system_memory']['percent']}")
            logger.info(f"- CPU ì‚¬ìš©ë¥ : {system_info['cpu']['percent']}")
            
            async with httpx.AsyncClient() as client:
                # í˜„ì¬ ì„œë²„ URL ë™ì  ìƒì„±
                host = "localhost"
                port = "8000"
                # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
                if os.getenv("RENDER") == "true":
                    # Render í™˜ê²½ì—ì„œëŠ” ì™¸ë¶€ URL ì‚¬ìš©
                    response = await client.get("https://construction-chatbot-api.onrender.com/ping")
                else:
                    # ê°œë°œ í™˜ê²½ì—ì„œëŠ” ë¡œì»¬ URL ì‚¬ìš©
                    response = await client.get(f"http://{host}:{port}/ping")
                
                logger.info(f"ìë™ í•‘ ì‘ë‹µ: {response.status_code}")
        except Exception as e:
            logger.error(f"ìë™ í•‘ ì—ëŸ¬: {e}")
            continue

# ì „ì—­ ë³€ìˆ˜ë¡œ íƒœìŠ¤í¬ ì €ì¥
background_tasks = set()
active_processes = set()  # í™œì„± í”„ë¡œì„¸ìŠ¤ ì¶”ì ì„ ìœ„í•œ ì„¸íŠ¸ ì¶”ê°€

@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ì‹¤í–‰ë˜ëŠ” ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬"""
    logger.info("ì„œë²„ ì‹œì‘: ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì‹œì‘")
    
    # ì´ˆê¸° ë²¡í„° DB ì´ˆê¸°í™” (force_rebuild=Falseë¡œ ì„¤ì •í•˜ì—¬ ê¸°ì¡´ ì„ë² ë”© ì¬ì‚¬ìš©)
    init_result = await initialize_vector_db(force_rebuild=False)
    logger.info(f"ë²¡í„° DB ì´ˆê¸°í™” ê²°ê³¼: {init_result}")
    
    # íŒŒì¼ ê°ì‹œ ì‹œì‘
    global last_modified_time
    if ENHANCED_FAQ_PATH.exists():
        last_modified_time = os.path.getmtime(ENHANCED_FAQ_PATH)
    
    # íŒŒì¼ ë³€ê²½ ê°ì§€ íƒœìŠ¤í¬ ì‹œì‘
    file_check_task = asyncio.create_task(check_file_changes())
    background_tasks.add(file_check_task)
    file_check_task.add_done_callback(background_tasks.discard)
    
    # ì„œë²„ í™œì„± ìƒíƒœ ìœ ì§€ë¥¼ ìœ„í•œ ìë™ í•‘ íƒœìŠ¤í¬ ì‹œì‘
    keep_alive_task = asyncio.create_task(keep_alive())
    background_tasks.add(keep_alive_task)
    keep_alive_task.add_done_callback(background_tasks.discard)
    
    logger.info("íŒŒì¼ ê°ì‹œ ë° ìë™ í•‘ ì‹œì‘ë¨")

@app.on_event("shutdown")
async def shutdown_event():
    """ì„œë²„ ì¢…ë£Œ ì‹œ ì‹¤í–‰ë˜ëŠ” ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬"""
    logger.info("ì„œë²„ ì¢…ë£Œ: ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ ë° í”„ë¡œì„¸ìŠ¤ ì •ë¦¬ ì‹œì‘")
    
    # ëª¨ë“  ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ ì·¨ì†Œ
    for task in background_tasks:
        task.cancel()
    
    # íƒœìŠ¤í¬ê°€ ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°
    if background_tasks:
        await asyncio.gather(*background_tasks, return_exceptions=True)
    
    # í™œì„± í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
    for process in active_processes:
        try:
            if process.poll() is None:  # í”„ë¡œì„¸ìŠ¤ê°€ ì•„ì§ ì‹¤í–‰ ì¤‘ì¸ ê²½ìš°
                process.terminate()  # ë¨¼ì € ì •ìƒ ì¢…ë£Œ ì‹œë„
                try:
                    process.wait(timeout=5)  # 5ì´ˆ ë™ì•ˆ ëŒ€ê¸°
                except subprocess.TimeoutExpired:
                    process.kill()  # ê°•ì œ ì¢…ë£Œ
        except Exception as e:
            logger.error(f"í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    logger.info("ì„œë²„ ì¢…ë£Œ: ëª¨ë“  ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ ë° í”„ë¡œì„¸ìŠ¤ ì •ë¦¬ ì™„ë£Œ")

# ìˆ˜ë™ ì´ˆê¸°í™”ìš© ì—”ë“œí¬ì¸íŠ¸ (í•„ìš”ì‹œ ì¬ì´ˆê¸°í™” ê°€ëŠ¥)
@app.get("/init-db")
async def manual_initialize_database():
    """ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    # ìˆ˜ë™ ì´ˆê¸°í™” ì‹œì—ëŠ” force_rebuild=Trueë¡œ ì„¤ì •
    success = await initialize_vector_db(force_rebuild=True)
    if success.get("success", False):
        return {"status": "success", "message": success.get("message", "ë°ì´í„°ë² ì´ìŠ¤ê°€ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")}
    else:
        return {"status": "error", "message": success.get("message", "ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")}

@app.post("/upload-excel")
async def upload_excel(file: UploadFile = File(...)):
    """Excel íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì €ì¥í•©ë‹ˆë‹¤."""
    try:
        # íŒŒì¼ í™•ì¥ì í™•ì¸
        if not file.filename.endswith(('.xlsx', '.xls')):
            raise HTTPException(status_code=400, detail="Excel íŒŒì¼(.xlsx ë˜ëŠ” .xls)ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        
        # íŒŒì¼ ì €ì¥ ê²½ë¡œ
        save_path = DOCS_DIR / "qa_pairs.xlsx"
        
        # íŒŒì¼ ì €ì¥
        content = await file.read()
        with open(save_path, "wb") as f:
            f.write(content)
        
        logger.info(f"Excel íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤: {save_path}")
        
        return JSONResponse(
            content={"message": "Excel íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. 'ì²˜ë¦¬í•˜ê¸°' ë²„íŠ¼ì„ ëˆŒëŸ¬ ë°ì´í„°ë¥¼ êµ¬ì¡°í™”í•´ì£¼ì„¸ìš”."},
            status_code=200
        )
    except Exception as e:
        logger.error(f"Excel íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"Excel íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}")

# process-excel ì—”ë“œí¬ì¸íŠ¸ ìˆ˜ì •
@app.post("/process-excel")
async def process_excel():
    """ì—…ë¡œë“œëœ FAQ ì—‘ì…€ íŒŒì¼ì„ ì²˜ë¦¬í•˜ê³  ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        qa_file_path = DOCS_DIR / "qa_pairs.xlsx"
        if not qa_file_path.exists():
            return JSONResponse(
                status_code=400,
                content={"success": False, "message": "ì²˜ë¦¬í•  ì—‘ì…€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."}
            )
        
        logger.info("FAQ êµ¬ì¡°í™” ì²˜ë¦¬ ì‹œì‘...")
        
        # ë¹„ë™ê¸° ì‘ì—…ìœ¼ë¡œ ë³€ê²½
        async def process_faq():
            try:
                script_path = Path(__file__).parent / "qa_allinone.py"
                
                if not script_path.exists():
                    raise FileNotFoundError(f"qa_allinone.py íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {script_path}")
                
                logger.info(f"qa_allinone.py ì‹¤í–‰ ì‹œì‘: {script_path}")
                
                # í™˜ê²½ì— ë”°ë¥¸ subprocess ì²˜ë¦¬ ë°©ì‹ ì„ íƒ
                is_windows = os.name == 'nt'
                is_render = os.getenv("RENDER") == "true"
                
                if is_windows and not is_render:
                    # Windows ê°œë°œ í™˜ê²½
                    logger.info("Windows ê°œë°œ í™˜ê²½ì—ì„œ ì‹¤í–‰")
                    process = subprocess.Popen(
                        [sys.executable, str(script_path)],
                        stdout=subprocess.PIPE,
                        stderr=subprocess.PIPE,
                        text=True,
                        encoding='utf-8',
                        cwd=str(Path(__file__).parent),
                        env={**os.environ, 'PYTHONPATH': str(Path(__file__).parent)}
                    )
                    
                    # í”„ë¡œì„¸ìŠ¤ë¥¼ í™œì„± í”„ë¡œì„¸ìŠ¤ ì„¸íŠ¸ì— ì¶”ê°€
                    active_processes.add(process)
                    
                    # ì‹¤ì‹œê°„ ë¡œê·¸ ì²˜ë¦¬
                    error_output = []
                    
                    # stdoutê³¼ stderrë¥¼ ë¹„ë™ê¸°ë¡œ ì½ê¸°
                    async def read_stream(stream, is_error=False):
                        while True:
                            line = stream.readline()
                            if not line and process.poll() is not None:
                                break
                            if line:
                                line = line.strip()
                                if is_error:
                                    error_output.append(line)
                                    logger.error(f"[qa_allinone.py] {line}")
                                else:
                                    logger.info(f"[qa_allinone.py] {line}")
                
                    # stdoutê³¼ stderrë¥¼ ë™ì‹œì— ì²˜ë¦¬
                    await asyncio.gather(
                        read_stream(process.stdout),
                        read_stream(process.stderr, is_error=True)
                    )
                    
                    # í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ëŒ€ê¸°
                    return_code = process.wait()
                    
                    # í”„ë¡œì„¸ìŠ¤ë¥¼ í™œì„± í”„ë¡œì„¸ìŠ¤ ì„¸íŠ¸ì—ì„œ ì œê±°
                    active_processes.discard(process)
                else:
                    # Render ë°°í¬ í™˜ê²½ ë˜ëŠ” Linux/Mac
                    logger.info("Render ë°°í¬ í™˜ê²½ ë˜ëŠ” Linux/Macì—ì„œ ì‹¤í–‰")
                    process = await asyncio.create_subprocess_exec(
                        sys.executable, str(script_path),
                        stdout=asyncio.subprocess.PIPE,
                        stderr=asyncio.subprocess.PIPE,
                        cwd=str(Path(__file__).parent),
                        env={**os.environ, 'PYTHONPATH': str(Path(__file__).parent)}
                    )
                    
                    # í”„ë¡œì„¸ìŠ¤ë¥¼ í™œì„± í”„ë¡œì„¸ìŠ¤ ì„¸íŠ¸ì— ì¶”ê°€
                    active_processes.add(process)
                    
                    # ì‹¤ì‹œê°„ ë¡œê·¸ ì²˜ë¦¬
                    error_output = []
                    
                    # stdoutê³¼ stderrë¥¼ ë¹„ë™ê¸°ë¡œ ì½ê¸°
                    async def read_stream(stream, is_error=False):
                        while True:
                            line = await stream.readline()
                            if not line and process.returncode is not None:
                                break
                            if line:
                                line = line.decode().strip()
                                if is_error:
                                    error_output.append(line)
                                    logger.error(f"[qa_allinone.py] {line}")
                                else:
                                    logger.info(f"[qa_allinone.py] {line}")
                    
                    # stdoutê³¼ stderrë¥¼ ë™ì‹œì— ì²˜ë¦¬
                    await asyncio.gather(
                        read_stream(process.stdout),
                        read_stream(process.stderr, is_error=True)
                    )
                    
                    # í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ëŒ€ê¸°
                    return_code = await process.wait()
                    
                    # í”„ë¡œì„¸ìŠ¤ë¥¼ í™œì„± í”„ë¡œì„¸ìŠ¤ ì„¸íŠ¸ì—ì„œ ì œê±°
                    active_processes.discard(process)
                
                logger.info(f"qa_allinone.py ì¢…ë£Œ ì½”ë“œ: {return_code}")
                
                if return_code != 0:
                    error_message = "\n".join(error_output) if error_output else "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜"
                    raise Exception(f"qa_allinone.py ì‹¤í–‰ ì‹¤íŒ¨ (ì¢…ë£Œ ì½”ë“œ: {return_code}): {error_message}")
                
                # ë²¡í„° DB ì´ˆê¸°í™”
                logger.info("FAQ êµ¬ì¡°í™” ì²˜ë¦¬ ì™„ë£Œ. ì´ì œ ìƒˆë¡œìš´ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")
                
                if not ENHANCED_FAQ_PATH.exists():
                    raise FileNotFoundError(f"êµ¬ì¡°í™”ëœ FAQ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {ENHANCED_FAQ_PATH}")
                
                if not load_enhanced_faq():
                    raise Exception("êµ¬ì¡°í™”ëœ FAQ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
                db_result = await initialize_vector_db(force_rebuild=True)
                if not db_result["success"]:
                    raise Exception(f"ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {db_result['message']}")
                
                return True
                
            except Exception as e:
                logger.error(f"FAQ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                import traceback
                logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
                return False
        
        # ë¹„ë™ê¸° ì‘ì—… ì‹œì‘
        task = asyncio.create_task(process_faq())
        
        # ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
        success = await task
        
        if success:
            return {"success": True, "message": "FAQ ì²˜ë¦¬ ë° ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."}
        else:
            return JSONResponse(
                status_code=500,
                content={"success": False, "message": "FAQ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë¡œê·¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."}
            )
        
    except Exception as e:
        logger.error(f"FAQ ì²˜ë¦¬ ì‹œì‘ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        import traceback
        logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        return JSONResponse(
            status_code=500,
            content={"success": False, "message": f"FAQ ì²˜ë¦¬ ì‹œì‘ ì¤‘ ì˜¤ë¥˜: {str(e)}"}
        )

# ì •ì  íŒŒì¼ ì„œë¹™ ì„¤ì •ì€ ëª¨ë“  API ì—”ë“œí¬ì¸íŠ¸ ì •ì˜ í›„ ë§¨ ë§ˆì§€ë§‰ì— ìœ„ì¹˜
if FRONTEND_BUILD_DIR.exists():
    app.mount("/", StaticFiles(directory=str(FRONTEND_BUILD_DIR), html=True), name="frontend")
    logger.info(f"Frontend static files mounted from {FRONTEND_BUILD_DIR}")
else:
    logger.warning(f"Frontend build directory not found at {FRONTEND_BUILD_DIR}. Static file serving is disabled.")

@app.get("/download-questions")
async def download_questions_excel():
    """ì €ì¥ëœ ì§ˆë¬¸ì„ Excel íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        # ì„ì‹œ íŒŒì¼ ìƒì„±
        with tempfile.NamedTemporaryFile(suffix='.xlsx', delete=False) as temp_file:
            temp_path = temp_file.name
        
        # ê¸°ì¡´ CSV íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
        if not QUESTIONS_FILE.exists():
            # íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¹ˆ ë°ì´í„°í”„ë ˆì„ ìƒì„±
            df = pd.DataFrame(columns=['timestamp', 'session_id', 'query', 'answer', 'similarity'])
        else:
            # CSV íŒŒì¼ ì½ê¸°
            try:
                df = pd.read_csv(QUESTIONS_FILE, encoding='utf-8-sig')
            except Exception as e:
                logger.error(f"CSV íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
                df = pd.DataFrame(columns=['timestamp', 'session_id', 'query', 'answer', 'similarity'])
        
        # ì›Œí¬ë¶ ìƒì„±
        wb = Workbook()
        ws = wb.active
        ws.title = "ì§ˆë¬¸ ë°ì´í„°"
        
        # í—¤ë” ì¶”ê°€
        headers = ['ì‹œê°„', 'ì„¸ì…˜ ID', 'ì§ˆë¬¸', 'ë‹µë³€', 'ìœ ì‚¬ë„']
        for col_num, header in enumerate(headers, 1):
            cell = ws.cell(row=1, column=col_num)
            cell.value = header
            # í—¤ë” ìŠ¤íƒ€ì¼ ì„¤ì •
            cell.font = cell.font.copy(bold=True)
        
        # ë°ì´í„° ì¶”ê°€
        for row_idx, row in df.iterrows():
            for col_idx, value in enumerate(row, 1):
                ws.cell(row=row_idx+2, column=col_idx, value=value)
        
        # ì—´ ë„ˆë¹„ ìë™ ì¡°ì •
        for col in ws.columns:
            max_length = 0
            column = col[0].column_letter
            for cell in col:
                if cell.value:
                    cell_length = len(str(cell.value))
                    if cell_length > max_length:
                        max_length = cell_length
            
            # ìµœëŒ€ ë„ˆë¹„ ì œí•œ (ë„ˆë¬´ ë„“ì§€ ì•Šê²Œ)
            adjusted_width = min(max_length + 2, 50)
            ws.column_dimensions[column].width = adjusted_width
        
        # íŒŒì¼ ì €ì¥
        wb.save(temp_path)
        
        # íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‘ë‹µ
        return FileResponse(
            path=temp_path,
            filename="saved_questions.xlsx",
            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
    except Exception as e:
        logger.error(f"Excel íŒŒì¼ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/download-enhanced-qa")
async def download_enhanced_qa(filename: str = Query("enhanced_qa_pairs.xlsx")):
    """êµ¬ì¡°í™”ëœ enhanced_qa_pairs.xlsx íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        if not ENHANCED_FAQ_PATH.exists():
            raise HTTPException(status_code=404, detail="enhanced_qa_pairs.xlsx íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return FileResponse(
            path=ENHANCED_FAQ_PATH,
            filename=filename,
            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
    except Exception as e:
        logger.error(f"enhanced_qa_pairs.xlsx ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))